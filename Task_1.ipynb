{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Implement Logistics Regression From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code implementation of the Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the Training Data, Features and Labels\n",
    "\n",
    "train_data = pd.read_csv(\"train_tfidf_features.csv\")\n",
    "train_raw = pd.read_csv(\"train.csv\")\n",
    "\n",
    "train_features = train_data.loc[:, '0':train_data.columns[-1]]\n",
    "train_label = train_data[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def loss(y, y_hat):\n",
    "    n = y.shape[0]\n",
    "    epsilon = 1e-15\n",
    "    J = np.sum(y * np.log(y_hat + epsilon) + (1 - y) * np.log(1 - y_hat + epsilon))\n",
    "    return -(1 / n) * J\n",
    "\n",
    "def gradients(X, y, y_hat):\n",
    "    m = X.shape[0]  # Assuming X is (bs, columns), m is the number of samples in the batch\n",
    "    \n",
    "    # Reshape y to match the shape of y_hat (if necessary)\n",
    "    if y_hat.shape != y.shape:\n",
    "        y = y.reshape(y_hat.shape)\n",
    "\n",
    "    dw = (1 / m) * np.dot(X.T, (y_hat - y))\n",
    "    db = (1 / m) * np.sum(y_hat - y)\n",
    "    return dw, db\n",
    "\n",
    "def train(X, y, bs, epochs, lr):\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        X = X.values\n",
    "    if isinstance(y, pd.Series):\n",
    "        y = y.values \n",
    "\n",
    "    rows, columns = X.shape      \n",
    "    w = np.zeros(columns)\n",
    "    b = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        for _ in range(rows//bs):\n",
    "            batch_indices = np.random.choice(rows, bs, replace=False)\n",
    "            X_batch = X[batch_indices, : ]\n",
    "            y_batch = y[batch_indices]\n",
    "            \n",
    "            Xw = np.dot(X_batch, w)\n",
    "            y_hat = sigmoid( Xw +b )   \n",
    "\n",
    "            current_loss = loss(y_batch, y_hat)\n",
    "            print(current_loss)\n",
    "\n",
    "            dw, db = gradients(X_batch, y_batch, y_hat)\n",
    "\n",
    "            w -= lr * dw\n",
    "            b -= lr * db            \n",
    "    return w, b\n",
    "\n",
    "def predict(X, w, b):\n",
    "    Xw = np.dot(X, w) + b\n",
    "    y_hat = sigmoid(Xw)\n",
    "    predictions = (y_hat >= 0.5).astype(int)  # Thresholding at 0.5 for binary classification\n",
    "    return predictions\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    correct = np.sum(y_true == y_pred)\n",
    "    total = len(y_true)\n",
    "    return correct / total\n",
    "\n",
    "def f1_calc(y_true, y_pred):\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fp = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    fn = np.sum((y_true == 0) & (y_pred == 1))\n",
    "\n",
    "    return tp / (tp + 0.5 * (fp + fn)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit values here:\n",
    "\n",
    "batch_size = 8\n",
    "epochs = 50\n",
    "learning_rate = 0.05\n",
    "\n",
    "# Training the model\n",
    "\n",
    "w, b = train(train_features, train_label, batch_size, epochs, learning_rate)\n",
    "\n",
    "print(w,b)\n",
    "\n",
    "train_predictions  = predict(train_features, w, b)\n",
    "\n",
    "f1_scores = f1_calc(train_label, train_predictions)\n",
    "print(f1_scores)\n",
    "\n",
    "acc = accuracy(train_label, train_predictions)\n",
    "\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Made Based On Logistic Regression Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the testing data\n",
    "\n",
    "test_data = pd.read_csv(\"test_tfidf_features.csv\")\n",
    "test_features = test_data.loc[:, '0':train_data.columns[-1]]\n",
    "\n",
    "# Applying our model to the Testing data\n",
    "\n",
    "test_predictions = predict(test_features, w, b)\n",
    "test_id = test_data[\"id\"]\n",
    "\n",
    "# Saving data to LogRed_Prediction.csv\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "        'id': test_id,\n",
    "        'label': test_predictions\n",
    "    })\n",
    "\n",
    "submission.to_csv('LogRed_Prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Comparison\n",
    "\n",
    "| Metric   | SKLearn | Our Model |\n",
    "|----------|---------|-----------|\n",
    "| F1 Score | 0.68927 | 0.69116   |\n",
    "| Accuracy | 0.74351 | 0.75686   |\n",
    "\n",
    "We adjusted the values of **Epoch**, **Batch Size**, and **Learning Rate** until our F1 Score and accuracy were marginally better than SKLearnâ€™s Logistic Regression model. This approach was taken to ensure that our model isn't overfitting to the training data, providing a more reliable performance on unseen data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
