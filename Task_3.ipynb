{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score, confusion_matrix\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score, cross_val_predict\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "from xgboost import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 Exploration Of Other Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create submission CSV\n",
    "def create_submission_csv(y_pred, test_id, model_name):\n",
    "    \"\"\"\n",
    "    Creates a CSV file for submission.\n",
    "    |\n",
    "    Parameters:\n",
    "    y_pred (array-like): Predicted labels.\n",
    "    test_id (array-like): IDs corresponding to the test set.\n",
    "    model_name (str): Name of the model used for predictions.\n",
    "    \n",
    "    Returns:\n",
    "    str: The name of the created CSV file.\n",
    "    \"\"\"\n",
    "    if len(y_pred) != len(test_id):\n",
    "        raise ValueError(\"Length of y_pred and test_id must be the same.\")\n",
    "    \n",
    "    submission = pd.DataFrame({\n",
    "        'id': test_id,\n",
    "        'label': y_pred\n",
    "    })\n",
    "    \n",
    "    submission_file = f'{model_name}.csv'\n",
    "    submission.to_csv(submission_file, index=False)\n",
    "    print(f\"Submission file created: {submission_file}\")\n",
    "    return submission_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_model(model, X, y, cv=5):\n",
    "    # Perform cross-validation\n",
    "    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "    f1_scores = cross_val_score(model, X, y, cv=skf, scoring='f1')\n",
    "    accuracy_scores = cross_val_score(model, X, y, cv=skf, scoring='accuracy')\n",
    "    recall_scores = cross_val_score(model, X, y, cv=skf, scoring='recall')\n",
    "    precision_scores = cross_val_score(model, X, y, cv=skf, scoring='precision')\n",
    "    y_pred_cv = cross_val_predict(model, X, y, cv=skf)\n",
    "\n",
    "    print(f\"Cross-Validation F1 Scores: {f1_scores}\")\n",
    "    print(f\"Cross-Validation Accuracy Scores: {accuracy_scores}\")\n",
    "    print(f\"Cross-Validation Recall Scores: {recall_scores}\")\n",
    "    print(f\"Cross-Validation Precision Scores: {precision_scores}\")\n",
    "\n",
    "    print(f\"Mean F1 Score: {np.mean(f1_scores)}\")\n",
    "    print(f\"Mean Accuracy: {np.mean(accuracy_scores)}\")\n",
    "    print(f\"Mean Recall: {np.mean(recall_scores)}\")\n",
    "    print(f\"Mean Precision: {np.mean(precision_scores)}\")\n",
    "    \n",
    "    # Confusion Matrix for Cross-Validation\n",
    "    cm = confusion_matrix(y, y_pred_cv)\n",
    "    print(f\"Cross-Validation Confusion Matrix:\\n{cm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "train_tfidf = pd.read_csv('train_tfidf_features.csv')\n",
    "test_tfidf = pd.read_csv('test_tfidf_features.csv')\n",
    "\n",
    "# Extract IDs from the datasets\n",
    "train_ids = train_data.iloc[:, 0]  # Assuming the ID is the first column\n",
    "test_ids = test_data[\"id\"]\n",
    "\n",
    "# Remove the first two columns from the TF-IDF features\n",
    "train_features = train_tfidf.iloc[:, 2:]\n",
    "test_features = test_tfidf.iloc[:, 2:]\n",
    "\n",
    "# Ensure the columns in test_features are in the same order and have the same names as in train_features\n",
    "common_columns = [col for col in train_features.columns if col in test_features.columns]\n",
    "train_features = train_features[common_columns]\n",
    "test_features = test_features[common_columns]\n",
    "\n",
    "# Define labels\n",
    "train_label = train_data[\"label\"]\n",
    "\n",
    "# Split the train set into train (80%) and validation (20%) sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_features, train_label, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Perform Grid Search to get the best performing parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Hyperparameter Tuning\n",
    "\n",
    "- **n_estimators**: \n",
    "  - Controls the number of boosting rounds.\n",
    "  - A higher number can lead to better performance but increases the risk of overfitting.\n",
    "  - Tested different values to find a balance between performance and overfitting.\n",
    "\n",
    "- **learning_rate**:\n",
    "  - Controls how much each tree contributes to the overall model.\n",
    "  - Lower learning rates can require more boosting rounds.\n",
    "  - Tested a range of values to optimize model convergence.\n",
    "\n",
    "- **max_depth**:\n",
    "  - Adjusted the depth of the trees to control the model's complexity.\n",
    "  - Experimented with different depths to balance model performance and complexity.\n",
    "\n",
    "- **subsample**:\n",
    "  - Set to 0.5 to randomly sample half of the data for each tree.\n",
    "  - Helps to prevent overfitting and improve generalization.\n",
    "\n",
    "- **gamma**:\n",
    "  - Adds regularization by only splitting nodes when the split results in a significant reduction in loss.\n",
    "  - Tuned to control the model's complexity and make it more conservative.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the base models\n",
    "# Define the initial XGBClassifier with given parameters\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.25,\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    max_depth=7,\n",
    "    subsample=0.5\n",
    ")\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [500, 700],\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'subsample': 0.5,\n",
    "    'gamma': [0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV for xgb_model\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_grid=param_grid,\n",
    "    scoring='f1',  # Optimize for F1 score\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    verbose=3,\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best parameters obtained from GridSearchCV for logistic regression\n",
    "best_params = {\n",
    "    'C': 1,\n",
    "    'class_weight': 'balanced',\n",
    "    'max_iter': 1000,  # Increased max_iter to allow for convergence\n",
    "    'penalty': 'l1',\n",
    "    'solver': 'liblinear',\n",
    "    'random_state': 42\n",
    "}\n",
    "simple_best_params = {\n",
    "    'C': 1,\n",
    "    'class_weight': 'balanced',\n",
    "    'max_iter': 100,  # Increased max_iter to allow for convergence\n",
    "    'penalty': 'l1',\n",
    "    'solver': 'liblinear',\n",
    "    'random_state': 42,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize other models for exploration\n",
    "knn_model = KNeighborsClassifier(n_neighbors=10)\n",
    "rf_model = RandomForestClassifier(random_state=42,n_estimators=1024)\n",
    "svc_model = SVC(probability=True, random_state=42, kernel='linear')\n",
    "nb_model = MultinomialNB(class_prior=[0.67,0.3])\n",
    "bb_model = BernoulliNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tried different methods for dimensionality reduction and feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = TruncatedSVD(n_components=2500)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_val_pca = pca.transform(X_val)\n",
    "X_test_pca = pca.transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = TruncatedSVD(n_components=600)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_val_pca = pca.transform(X_val)\n",
    "X_test_pca = pca.transform(test_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying PCA to reduce dimensions with the determined number of components\n",
    "pca2 = PCA(n_components=400)\n",
    "X_train_pca2 = pca.fit_transform(X_train)\n",
    "X_val_pca2 = pca.transform(X_val)\n",
    "X_test_pca2 = pca.transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca1 = PCA(n_components=1000)\n",
    "X_train_pca1 = pca1.fit_transform(X_train)\n",
    "X_val_pca1 = pca1.transform(X_val)\n",
    "X_test_pca1 = pca1.transform(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sum of explained variance for each of the different parameters are below 50%. It shows that reducing the parameters further is not the right direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {\n",
    "    'C': 0.9,\n",
    "    'class_weight': {0: 1, 1: 2.9},\n",
    "    'max_iter': 100,  \n",
    "    'penalty': 'l1',\n",
    "    'solver': 'liblinear',\n",
    "    'random_state': 42,\n",
    "}\n",
    "\n",
    "# Initialize the base logistic regression model with the best parameters\n",
    "logreg_model = LogisticRegression(**best_params)\n",
    "nb_model = MultinomialNB(class_prior=[0.67,0.33])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Best VotingClassifier\n",
    "voting_model = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('logreg', logreg_model),\n",
    "        ('nb', nb_model)\n",
    "    ],\n",
    "    voting='soft',  # 'soft' uses predicted probabilities for averaging\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation F1 Scores: [0.64140218 0.63835878 0.64555766 0.6491897  0.62618596]\n",
      "Cross-Validation Accuracy Scores: [0.72472727 0.72436364 0.72717352 0.73226628 0.71335031]\n",
      "Cross-Validation Recall Scores: [0.64231499 0.63472486 0.64862298 0.64672365 0.62678063]\n",
      "Cross-Validation Precision Scores: [0.64049196 0.64203455 0.64252117 0.65167464 0.62559242]\n",
      "Mean F1 Score: 0.6401388552754737\n",
      "Mean Accuracy: 0.7243762029167631\n",
      "Mean Recall: 0.6398334207315864\n",
      "Mean Precision: 0.640462946407381\n",
      "Cross-Validation Confusion Matrix:\n",
      "[[6588 1892]\n",
      " [1897 3370]]\n",
      "Validation F1 Score: 0.627227910504361\n",
      "Validation Recall: 0.6440809968847352\n",
      "Validation Precision: 0.6112342941611234\n",
      "Submission file created: voting_predictions.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'voting_predictions.csv'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_model.fit(X_train, y_train)\n",
    "\n",
    "cross_validate_model(voting_model, X_train, y_train)\n",
    "# Aggregate predictions on the validation set\n",
    "y_pred_val = voting_model.predict(X_val)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_f1_score = f1_score(y_val, y_pred_val)\n",
    "val_accuracy = accuracy_score(y_val, y_pred_val)\n",
    "val_recall = recall_score(y_val, y_pred_val)\n",
    "val_precision = precision_score(y_val, y_pred_val)\n",
    "print(f\"Validation F1 Score: {val_f1_score}\")\n",
    "print(f\"Validation Recall: {val_recall}\")\n",
    "print(f\"Validation Precision: {val_precision}\")\n",
    "\n",
    "# Predict on the test set using the VotingClassifier\n",
    "y_pred_test = voting_model.predict(test_features)\n",
    "\n",
    "\n",
    "# Save the test set predictions to a CSV file\n",
    "create_submission_csv(y_pred_test, test_ids, \"voting_predictions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation Metrics\n",
    "\n",
    "## Cross-Validation Scores\n",
    "\n",
    "- **F1 Scores**: [0.6414, 0.6384, 0.6456, 0.6492, 0.6262]\n",
    "- **Accuracy Scores**: [0.7247, 0.7244, 0.7272, 0.7323, 0.7134]\n",
    "- **Recall Scores**: [0.6423, 0.6347, 0.6486, 0.6467, 0.6268]\n",
    "- **Precision Scores**: [0.6405, 0.6420, 0.6425, 0.6517, 0.6256]\n",
    "\n",
    "## Mean Cross-Validation Scores\n",
    "\n",
    "- **Mean F1 Score**: 0.6401\n",
    "- **Mean Accuracy**: 0.7244\n",
    "- **Mean Recall**: 0.6398\n",
    "- **Mean Precision**: 0.6405\n",
    "\n",
    "## Cross-Validation Confusion Matrix\n",
    "\n",
    "| True Negative | False Positive | False Negative | True Positive |\n",
    "|:-------------:|:--------------:|:--------------:|:-------------:|\n",
    "| 6588          | 1892           | 1897           | 3370          |\n",
    "\n",
    "## Validation Metrics\n",
    "\n",
    "- **Validation F1 Score**: 0.6272\n",
    "- **Validation Recall**: 0.6441\n",
    "- **Validation Precision**: 0.6112\n",
    "\n",
    "## Submission\n",
    "\n",
    "- Submission file created: **`voting_predictions.csv`**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file created: voting_full_predictions.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'voting_full_predictions.csv'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the VotingClassifier on the PCA-transformed full training data\n",
    "voting_model.fit(train_features, train_label)\n",
    "\n",
    "\n",
    "# Predict on the test set using the retrained VotingClassifier\n",
    "y_pred_test = voting_model.predict(test_features)\n",
    "\n",
    "# Save the test set predictions to a CSV file\n",
    "create_submission_csv(y_pred_test, test_ids, \"voting_full_predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final F1 Score on Kaggle\n",
    "\n",
    "- **Public Scoreboard**: 0.71755\n",
    "- **Private Scoreboard**: 0.70612\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
